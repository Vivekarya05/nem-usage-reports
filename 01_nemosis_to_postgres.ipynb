{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 ‚Äì Ingest 5-Minute NEM Demand Data (NEMOSIS ‚Üí PostgreSQL)\n",
    "\n",
    "## Purpose\n",
    "\n",
    "This notebook builds the **raw operational data layer** for the project.\n",
    "\n",
    "We extract 5-minute regional demand data from the AEMO public archive using the **NEMOSIS** Python library, then store a clean version of this data in **PostgreSQL** for downstream analytics and dashboards.\n",
    "\n",
    "## How it fits into the overall methodology\n",
    "\n",
    "This is **Layer 1 ‚Äì Data Ingestion** in the project pipeline:\n",
    "\n",
    "1. **Raw 5-min data** (this notebook)  \n",
    "2. **Daily & monthly usage analytics** (later notebooks)  \n",
    "3. **Operational insights** ‚Äì peak events, low-demand events, weekday profiles  \n",
    "4. **Region-level KPIs & Power BI dashboards**\n",
    "\n",
    "All later analysis notebooks assume this base table exists in Postgres:\n",
    "\n",
    "- `dispatch_region_5min`  \n",
    "  - `settlement_ts` ‚Äì 5-min timestamp  \n",
    "  - `region_id` ‚Äì NEM region (NSW1, QLD1, etc.)  \n",
    "  - `total_demand` ‚Äì operational demand in MW\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 ‚Äì Load Environment Variables and Connect to PostgreSQL\n",
    "\n",
    "### Why we are doing this\n",
    "\n",
    "Before we fetch any data, we need a **reliable connection** to our database so that:\n",
    "\n",
    "- All ingested data is stored in a central, queryable location.\n",
    "- Analytics notebooks and BI tools (e.g. Power BI) can all point to the same source.\n",
    "- Credentials are managed securely via a `.env` file instead of hard-coding them.\n",
    "\n",
    "This step ensures the notebook can talk to PostgreSQL using the correct host, port, user, and database name.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected OK: [(1,)]\n"
     ]
    }
   ],
   "source": [
    " import os\n",
    "from sqlalchemy import create_engine, text\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "# Load .env\n",
    "env_path = Path().resolve() / \".env\"\n",
    "load_dotenv(env_path, override=True)\n",
    "\n",
    "DB_USER = os.getenv(\"DB_USER\")\n",
    "DB_PASSWORD = os.getenv(\"DB_PASSWORD\")\n",
    "DB_HOST = os.getenv(\"DB_HOST\")\n",
    "DB_PORT = os.getenv(\"DB_PORT\")\n",
    "DB_NAME = os.getenv(\"DB_NAME\")\n",
    "\n",
    "# Create engine\n",
    "engine = create_engine(\n",
    "    f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    ")\n",
    "\n",
    "# Test connection\n",
    "with engine.connect() as conn:\n",
    "    print(\"Connected OK:\", conn.execute(text(\"SELECT 1;\")).fetchall())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 ‚Äì Define NEMOSIS Table and Date Range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why we are doing this\n",
    "\n",
    "The AEMO operational data is split into many tables and time slices.\n",
    "For this project we care about **5-minute regional demand**, which lives in the NEMOSIS table:\n",
    "\n",
    "- `DISPATCHREGIONSUM`\n",
    "\n",
    "We also want a **fixed analysis window** (e.g. Jan 2025 to Oct 2025), so that:\n",
    "\n",
    "- All downstream analytics use a consistent timeframe.\n",
    "- The project is reproducible: same code ‚Üí same time window ‚Üí same results.\n",
    "\n",
    "This step simply sets up the configuration for which **table**, **regions**, and **date range** we are ingesting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 ‚Äì Download 5-Minute Regional Demand with NEMOSIS (Month-by-Month)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "The NEM archive is large, and downloading everything in one shot is:\n",
    "\n",
    "- Slow\n",
    "- Memory-heavy\n",
    "- Hard to debug\n",
    "\n",
    "Instead, we follow a **monthly ingestion pattern**:\n",
    "\n",
    "1. Loop month-by-month between the start and end dates.\n",
    "2. For each month, use NEMOSIS `dynamic_data_compiler()` to fetch `DISPATCHREGIONSUM`.\n",
    "3. Keep only the fields we need for this project:\n",
    "   - `SETTLEMENTDATE` (5-minute timestamp)\n",
    "   - `REGIONID` (NEM region)\n",
    "   - `TOTALDEMAND` (MW)\n",
    "4. Append each month‚Äôs data into a single DataFrame.\n",
    "\n",
    "This pattern is closer to what a **production ETL pipeline** would do: chunked, repeatable ingestion instead of a single massive pull.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- NEMOSIS DATA CHECK (ALL REGIONS, NO DB) ---\n",
      "\n",
      "üìÖ Fetching 2025/01/01 00:00:00 ‚Üí 2025/01/31 23:59:59 for DISPATCHREGIONSUM ...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched: 44635\n",
      "   üåè Regions in this chunk: ['NSW1' 'QLD1' 'SA1' 'TAS1' 'VIC1']\n",
      "   üìä Sample rows:\n",
      "       SETTLEMENTDATE REGIONID  DISPATCHINTERVAL  INTERVENTION  TOTALDEMAND  \\\n",
      "0 2025-01-01 00:05:00     NSW1       20241231241             0      7251.07   \n",
      "1 2025-01-01 00:05:00     QLD1       20241231241             0      6444.99   \n",
      "2 2025-01-01 00:05:00      SA1       20241231241             0      1379.47   \n",
      "\n",
      "   AVAILABLEGENERATION  AVAILABLELOAD  DEMANDFORECAST  DISPATCHABLEGENERATION  \\\n",
      "0          12034.75116            220             -28                 7612.94   \n",
      "1           9145.64397            420             -42                 5965.77   \n",
      "2           2473.89912            412              12                 1000.90   \n",
      "\n",
      "   DISPATCHABLELOAD  ...  RAISE6SECLOCALDISPATCH  INITIALSUPPLY  \\\n",
      "0               6.0  ...                   55.00     7307.44285   \n",
      "1               0.0  ...                  251.00     6506.42244   \n",
      "2               0.0  ...                  129.72     1399.75850   \n",
      "\n",
      "   CLEAREDSUPPLY  LOWERREGLOCALDISPATCH  RAISEREGLOCALDISPATCH  \\\n",
      "0        7284.11                    5.0                  10.00   \n",
      "1        6458.03                   15.0                  29.00   \n",
      "2        1384.52                   10.0                  78.29   \n",
      "\n",
      "   TOTALINTERMITTENTGENERATION  DEMAND_AND_NONSCHEDGEN       UIGF  \\\n",
      "0                     92.33276              7376.44276  852.75116   \n",
      "1                    105.78200              6563.81200  411.64397   \n",
      "2                      0.00000              1384.52000  673.89912   \n",
      "\n",
      "   SEMISCHEDULE_CLEAREDMW  SEMISCHEDULE_COMPLIANCEMW  \n",
      "0               852.75116                        0.0  \n",
      "1               411.64397                        0.0  \n",
      "2               673.89912                        0.0  \n",
      "\n",
      "[3 rows x 27 columns]\n",
      "\n",
      "üìÖ Fetching 2025/02/01 00:00:00 ‚Üí 2025/02/28 23:59:59 for DISPATCHREGIONSUM ...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched: 40315\n",
      "   üåè Regions in this chunk: ['NSW1' 'QLD1' 'SA1' 'TAS1' 'VIC1']\n",
      "   üìä Sample rows:\n",
      "       SETTLEMENTDATE REGIONID  DISPATCHINTERVAL  INTERVENTION  TOTALDEMAND  \\\n",
      "0 2025-02-01 00:05:00     NSW1       20250131241             0      7133.42   \n",
      "1 2025-02-01 00:05:00     QLD1       20250131241             0      6805.70   \n",
      "2 2025-02-01 00:05:00      SA1       20250131241             0      1547.89   \n",
      "\n",
      "   AVAILABLEGENERATION  AVAILABLELOAD  DEMANDFORECAST  DISPATCHABLEGENERATION  \\\n",
      "0          12463.56264            216             -53                 7070.74   \n",
      "1           9798.02858            404             -31                 6211.96   \n",
      "2           3079.17831            406               1                 1465.18   \n",
      "\n",
      "   DISPATCHABLELOAD  ...  RAISE6SECLOCALDISPATCH  INITIALSUPPLY  \\\n",
      "0               8.0  ...                   87.79     7246.82716   \n",
      "1               0.0  ...                  224.07     6855.16790   \n",
      "2               0.0  ...                   84.00     1568.22545   \n",
      "\n",
      "   CLEAREDSUPPLY  LOWERREGLOCALDISPATCH  RAISEREGLOCALDISPATCH  \\\n",
      "0        7214.17                   30.0                  42.67   \n",
      "1        6823.96                   58.0                  76.00   \n",
      "2        1547.85                   20.0                  39.33   \n",
      "\n",
      "   TOTALINTERMITTENTGENERATION  DEMAND_AND_NONSCHEDGEN        UIGF  \\\n",
      "0                     95.92047              7310.09047  1621.56264   \n",
      "1                     53.56400              6877.52400   515.02858   \n",
      "2                      0.00000              1547.85000  1383.17831   \n",
      "\n",
      "   SEMISCHEDULE_CLEAREDMW  SEMISCHEDULE_COMPLIANCEMW  \n",
      "0              1621.56264                        0.0  \n",
      "1               515.02858                        0.0  \n",
      "2              1383.17832                        0.0  \n",
      "\n",
      "[3 rows x 27 columns]\n",
      "\n",
      "--- NEMOSIS CHECK COMPLETE ---\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from nemosis import dynamic_data_compiler\n",
    "\n",
    "# ---------- SETTINGS (EDIT DATES IF YOU WANT) ----------\n",
    "table_name = \"DISPATCHREGIONSUM\"\n",
    "raw_data_cache_dir = \"data_raw\"\n",
    "\n",
    "# test for 1‚Äì2 months only to keep it light\n",
    "start_date = datetime(2025, 1, 1, 0, 0, 0)\n",
    "end_date   = datetime(2025, 3, 1, 0, 0, 0)   # not included (so Jan + Feb)\n",
    "\n",
    "os.makedirs(raw_data_cache_dir, exist_ok=True)\n",
    "\n",
    "current = start_date\n",
    "\n",
    "print(\"\\n--- NEMOSIS DATA CHECK (ALL REGIONS, NO DB) ---\")\n",
    "\n",
    "while current < end_date:\n",
    "    # month boundaries\n",
    "    next_month = (current.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "    end_of_month = next_month - timedelta(seconds=1)\n",
    "\n",
    "    start_str = current.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "    end_str   = end_of_month.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "    print(f\"\\nüìÖ Fetching {start_str} ‚Üí {end_str} for {table_name} ...\")\n",
    "\n",
    "    try:\n",
    "        df = dynamic_data_compiler(\n",
    "            start_str,\n",
    "            end_str,\n",
    "            table_name,\n",
    "            raw_data_cache_dir,   # raw_data_cache\n",
    "        )\n",
    "\n",
    "        print(\"   ‚úÖ Rows fetched:\", len(df))\n",
    "\n",
    "        if len(df) > 0:\n",
    "            # show region IDs available\n",
    "            if \"REGIONID\" in df.columns:\n",
    "                print(\"   üåè Regions in this chunk:\", df[\"REGIONID\"].unique())\n",
    "            else:\n",
    "                print(\"   ‚ö†Ô∏è 'REGIONID' column not found. Columns are:\")\n",
    "                print(\"      \", df.columns.tolist())\n",
    "\n",
    "            # show a tiny sample\n",
    "            print(\"   üìä Sample rows:\")\n",
    "            print(df.head(3))\n",
    "\n",
    "        del df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error for {current.strftime('%B %Y')}: {e}\")\n",
    "\n",
    "    current = next_month\n",
    "\n",
    "print(\"\\n--- NEMOSIS CHECK COMPLETE ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 ‚Äì Build `dispatch_region_5min` Table in PostgreSQL\n",
    "\n",
    "### Why we are doing this\n",
    "\n",
    "At this point we have a list of monthly DataFrames in memory.  \n",
    "To make the data useful for analytics and dashboards, we:\n",
    "\n",
    "1. **Concatenate** all monthly chunks into a single DataFrame.\n",
    "2. **Optionally drop an old table** so we don‚Äôt duplicate rows during testing.\n",
    "3. **Create a clean base table** in PostgreSQL:\n",
    "   - One row per 5-minute interval per region.\n",
    "   - Columns: timestamp, region, total demand.\n",
    "4. Validate with a quick preview and row count.\n",
    "\n",
    "This `dispatch_region_5min` table is the **foundation for all later notebooks** (daily, monthly, peaks, low demand, region comparison).\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ DB connection OK: [(1,)]\n",
      "üßπ Dropped old table dispatch_region_5min (if existed).\n",
      "\n",
      "--- Starting NEMOSIS ‚Üí Postgres Ingestion (ALL REGIONS) ---\n",
      "\n",
      "üìÖ Fetching 2025/01/01 00:00:00 ‚Üí 2025/01/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month January 2025.\n",
      "\n",
      "üìÖ Fetching 2025/02/01 00:00:00 ‚Üí 2025/02/28 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 40315\n",
      "   ‚úÖ Inserted 40315 rows for month February 2025.\n",
      "\n",
      "üìÖ Fetching 2025/03/01 00:00:00 ‚Üí 2025/03/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month March 2025.\n",
      "\n",
      "üìÖ Fetching 2025/04/01 00:00:00 ‚Üí 2025/04/30 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 43195\n",
      "   ‚úÖ Inserted 43195 rows for month April 2025.\n",
      "\n",
      "üìÖ Fetching 2025/05/01 00:00:00 ‚Üí 2025/05/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month May 2025.\n",
      "\n",
      "üìÖ Fetching 2025/06/01 00:00:00 ‚Üí 2025/06/30 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 43195\n",
      "   ‚úÖ Inserted 43195 rows for month June 2025.\n",
      "\n",
      "üìÖ Fetching 2025/07/01 00:00:00 ‚Üí 2025/07/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month July 2025.\n",
      "\n",
      "üìÖ Fetching 2025/08/01 00:00:00 ‚Üí 2025/08/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month August 2025.\n",
      "\n",
      "üìÖ Fetching 2025/09/01 00:00:00 ‚Üí 2025/09/30 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 43195\n",
      "   ‚úÖ Inserted 43195 rows for month September 2025.\n",
      "\n",
      "üìÖ Fetching 2025/10/01 00:00:00 ‚Üí 2025/10/31 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 44635\n",
      "   ‚úÖ Inserted 44635 rows for month October 2025.\n",
      "\n",
      "üìÖ Fetching 2025/11/01 00:00:00 ‚Üí 2025/11/30 23:59:59 for DISPATCHREGIONSUM (ALL REGIONS)...\n",
      "INFO: Compiling data for table DISPATCHREGIONSUM\n",
      "INFO: Downloading data for table DISPATCHREGIONSUM, year 2025, month 11\n",
      "WARNING: PUBLIC_ARCHIVE#DISPATCHREGIONSUM#FILE01#202511010000 not downloaded\n",
      "WARNING: Loading data from data_raw/PUBLIC_ARCHIVE#DISPATCHREGIONSUM#FILE01#202511010000.feather failed.\n",
      "INFO: Returning DISPATCHREGIONSUM.\n",
      "   ‚úÖ Rows fetched this month: 0\n",
      "   ‚ÑπÔ∏è No rows for this month, skipping.\n",
      "\n",
      "--- Ingestion complete! ---\n",
      "üì¶ Total rows inserted into dispatch_region_5min: 437710\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "from nemosis import dynamic_data_compiler\n",
    "\n",
    "# ---------- 1. POSTGRES SETTINGS ----------\n",
    "DB_USER = \"vivekarya\"          # change if your username is different\n",
    "DB_PASSWORD = \"Ap28bf9456\" #\n",
    "DB_HOST = \"localhost\"\n",
    "DB_PORT = 5432\n",
    "DB_NAME = \"postgres\"  \n",
    "\n",
    "def get_engine():\n",
    "    url = f\"postgresql+psycopg2://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}\"\n",
    "    return create_engine(url)\n",
    "\n",
    "# ---------- 2. NEMOSIS SETTINGS ----------\n",
    "table_name = \"DISPATCHREGIONSUM\"\n",
    "raw_data_cache_dir = \"data_raw\"\n",
    "os.makedirs(raw_data_cache_dir, exist_ok=True)\n",
    "\n",
    "# Use a small test range first (you can change later)\n",
    "start_date = datetime(2025, 1, 1, 0, 0, 0)\n",
    "end_date = datetime(2025, 11, 10, 0, 0, 0)  # Jan + Feb 2025\n",
    "\n",
    "def main():\n",
    "    engine = get_engine()\n",
    "\n",
    "    # Test DB connection\n",
    "    with engine.connect() as conn:\n",
    "        result_1=conn.execute(text(\"SELECT 1;\")).fetchall()\n",
    "        print(\"‚úÖ DB connection OK:\", result_1)\n",
    "\n",
    "    # (Optional) Clear old table so you don't duplicate rows during testing\n",
    "    with engine.connect() as conn:\n",
    "        conn.execute(text(\"DROP TABLE IF EXISTS dispatch_region_5min;\"))\n",
    "     ##execute(text(\"SELECT 1;\")).fetchall()\n",
    "        print(\"üßπ Dropped old table dispatch_region_5min (if existed).\")\n",
    "\n",
    "    current = start_date\n",
    "\n",
    "    print(\"\\n--- Starting NEMOSIS ‚Üí Postgres Ingestion (ALL REGIONS) ---\")\n",
    "\n",
    "    total_inserted = 0\n",
    "\n",
    "    while current < end_date:\n",
    "        # Month boundaries\n",
    "        next_month = (current.replace(day=28) + timedelta(days=4)).replace(day=1)\n",
    "        end_of_month = next_month - timedelta(seconds=1)\n",
    "\n",
    "        start_str = current.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "        end_str   = end_of_month.strftime(\"%Y/%m/%d %H:%M:%S\")\n",
    "\n",
    "        print(f\"\\nüìÖ Fetching {start_str} ‚Üí {end_str} for {table_name} (ALL REGIONS)...\")\n",
    "\n",
    "        try:\n",
    "            # 1Ô∏è‚É£ Fetch from NEMOSIS\n",
    "            df = dynamic_data_compiler(\n",
    "                start_str,\n",
    "                end_str,\n",
    "                table_name,\n",
    "                raw_data_cache_dir,   # raw_data_cache\n",
    "            )\n",
    "\n",
    "            print(f\"   ‚úÖ Rows fetched this month: {len(df)}\")\n",
    "\n",
    "            if len(df) == 0:\n",
    "                print(\"   ‚ÑπÔ∏è No rows for this month, skipping.\")\n",
    "                current = next_month\n",
    "                continue\n",
    "\n",
    "            # 2Ô∏è‚É£ Make sure REGIONID exists\n",
    "            if \"REGIONID\" not in df.columns:\n",
    "                print(\"   ‚ö†Ô∏è 'REGIONID' column missing, available columns:\")\n",
    "                print(\"      \", df.columns.tolist())\n",
    "                current = next_month\n",
    "                continue\n",
    "\n",
    "            # 3Ô∏è‚É£ Keep only columns we need\n",
    "            df[\"SETTLEMENTDATE\"] = pd.to_datetime(df[\"SETTLEMENTDATE\"])\n",
    "            df_clean = df[[\"SETTLEMENTDATE\", \"REGIONID\", \"TOTALDEMAND\"]].copy()\n",
    "\n",
    "            # 4Ô∏è‚É£ Rename columns for DB\n",
    "            df_clean = df_clean.rename(columns={\n",
    "                \"SETTLEMENTDATE\": \"settlement_ts\",\n",
    "                \"REGIONID\": \"region_id\",\n",
    "                \"TOTALDEMAND\": \"total_demand\",\n",
    "                \n",
    "            })\n",
    "\n",
    "            # 5Ô∏è‚É£ Insert into Postgres\n",
    "            df_clean.to_sql(\n",
    "                \"dispatch_region_5min\",\n",
    "                engine,\n",
    "                if_exists=\"append\",\n",
    "                index=False,\n",
    "            )\n",
    "\n",
    "            print(f\"   ‚úÖ Inserted {len(df_clean)} rows for month {current.strftime('%B %Y')}.\")\n",
    "            total_inserted += len(df_clean)\n",
    "\n",
    "            # free memory\n",
    "            del df\n",
    "            del df_clean\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error while processing {current.strftime('%B %Y')}: {e}\")\n",
    "\n",
    "        current = next_month\n",
    "\n",
    "    print(\"\\n--- Ingestion complete! ---\")\n",
    "    print(f\"üì¶ Total rows inserted into dispatch_region_5min: {total_inserted}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ CSV exported: dispatch_region_5min.csv\n",
      "Rows exported: 1313130\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Pull everything from Postgres into one dataframe\n",
    "df_export = pd.read_sql(\"\"\"\n",
    "    SELECT *\n",
    "    FROM dispatch_region_5min\n",
    "    ORDER BY settlement_ts, region_id;\n",
    "\"\"\", engine)\n",
    "\n",
    "# Export to CSV\n",
    "df_export.to_csv(\"dispatch_region_5min.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ CSV exported: dispatch_region_5min.csv\")\n",
    "print(\"Rows exported:\", len(df_export))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
